\documentclass[11pt, a4paper]{article}


\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage[utf8]{inputenc}
\usepackage[font=footnotesize, labelfont=bf]{caption}
\usepackage{csquotes}
\usepackage{placeins}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{amsmath}
\usepackage[margin=2.5cm, top=2.0cm]{geometry}
\usepackage{hyperref}
\usepackage{tabularx}
\hypersetup{
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=black
}
\usepackage[backend=biber, sorting=none]{biblatex}
\addbibresource{draft.bib}

\newcommand{\code}[1]{\texttt{#1}}
\linespread{1.3}

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000
\interfootnotelinepenalty=10000

\title{Bibliography
management:
\texttt{biblatex}
package}


\author{Krzytsztof
Wiśniewski}
\date{
}


\begin{document}
  \begin{titlepage}
    \centering


    \Large \textbf{UNIWERSYTET GDAŃSKI}\\ \textbf{WYDZIAŁ MATEMATYKI, FIZYKI I
    INFORMATYKI}

    \vspace{2.5cm}


    \large \textbf{Krzysztof Wiśniewski}\\ \textbf{numer albumu: 274276}

    \vspace{1.5cm}
    \raggedright \small Kierunek studiów: Bioinformatyka\\ Specjalność: Ogólna

    \vspace{1.5cm}


    \centering
    \Large \textbf{Optymalizacja oprogramowania do detekcji splątania kwantowego}

    \vfill


    \raggedleft \normalsize Praca licencjacka\\ wykonana\\ pod kierunkiem\\ dr hab. Marcina
    Wieśniaka, prof. UG\\

    \vfill


    \centering
    \large Gdańsk 2023
  \end{titlepage}
  \newpage


  \tableofcontents
  \newpage


  \begin{sloppypar}
    \section{Wstęp}
    Closest Separable State Finder (CSSFinder) jest programem pozwalającym na detekcję splątania
    kwantowego układu oraz określenie jak silnie owe splątanie jest. Opiera się on na
    dostosowanym algorytmie Elmera G. Gilberta\cite{Lindemann_Gilbert}, pozwalającym na wyliczenie
    przybliżonej wartości odległości Hilberta-Schmidta (ang. Hilberta-Schmidta distance,
    HSD) pomiędzy stanem kwantowym a zbiorem stanów separowanych. Działanie tego
    algorytmu zostało opisane w pracy `Hilbert-Schmidt distance and entanglement witnessing'
    której autorami byli Palash Pandya, Omer Sakarya i Marcin Wieśniak\cite{MW_Hilbert_Schmidt_distance}.

    Dr hab. Marcin Wieśniak, prof. UG, utworzył implementację algorytmu CSSF w języku
    Python\cite{Python_Language}, wykorzystując bibliotekę NumPy\cite{NumPy_Article} do przeprowadzania
    koniecznych obliczeń macierzowych. Wybór ten był podyktowany możliwościami
    oferowanymi przez taki zestaw narzędzi. Pozwalały one w szybki sposób stworzyć prosty
    kod, zdolny by relatywnie wydajnie przeprowadzać obliczenia na wszystkich
    najpopularniejszych systemach dla komputerów stacjonarnych.

    Zalety języka Python są powszechnie dostrzegane zarówno przez środowiska akademickie,
    jak i komercyjne, co wyraźnie widać w zestawieniach takich jak wydane przez GitHub, Inc.
    `The top programming languages' (2022)\cite{GitHub_Top_languages}. Język Python
    plasuje się w nim na drugim miejscu.

    Alternatywy w postaci języków C, C++ czy Fortran wymagałyby większej ilości bardziej
    skomplikowanego kodu, jednocześnie zmuszając do ręcznego skompletowania systemu
    budowania, bibliotek oraz zastosowania dedykowanych rozwiązań dla każdego systemu operacyjnego,
    a przeprowadzanie obliczeń byłoby utrudnione, ze względu na utrudniony dostęp do wyspecjalizowanych
    bibliotek.

    \subsection{Działanie programu}
    Oryginalny program i jego re-implementacje posiadają praktycznie identyczną zasadę działania
    i tylko szczegóły dotyczące implementacji i sposobu interakcji z nim zmieniły się. Z
    tego względu w dalszej części tekstu sposób działania programu będzie opisywany bez
    rozgraniczenia na wersję oryginalną i re-implementacje.

    Program jako dane wejściowe przyjmuje macierz gęstości opisującą pewien stan
    $\rho_{0}$ układu kwantowego. Macierz ta jest opisem matematycznym stanu cząstki lub
    układu cząstek w mechanice kwantowej. Wartość własna takiej macierzy musi być równa
    zero, a ślad musi być równy jeden.

    W określonych wypadkach program jest w stanie wydedukować wymiary podukładów i ich liczbę.
    Możliwe, a czasami wymagane, jest jawne podanie tych parametrów.

    Dla macierzy wejściowej dobierana jest macierz $\rho_{1}$ opisująca stan separowalny.

    Stan separowalny, zwany jest także stanem niesplątanym. W przypadku stanów czystych
    jest on stanem kwantowym systemu złożonego, który można opisać jako produkt tensorowy
    stanów poszczególnych podukładów. Innymi słowy, jeżeli mamy dwa podukłady A i B,
    stan $|\psi \rangle$ jest stanem separowalnym, jeżeli można go zapisać jako $|\psi\rangle
    = |\phi\rangle \otimes |\chi\rangle$, gdzie $| \phi\rangle$ to stan podukładu A i
    $|\chi\rangle$ to stan podukładu B.

    Stan splątany to specyficzny stan w mechanice kwantowej, w którym dwa lub więcej systemów
    kwantowych (na przykład cząstki) są tak ze sobą powiązane, że stan jednego systemu
    jest ściśle zależny od stanu drugiego.

    Stan splątany to taki, którego nie da się zapisać w sposób przedstawiony powyżej, na
    przykład
    $|\psi\rangle = |\phi_{0}\rangle \otimes |\chi_{0}\rangle + |\phi_{1}\rangle \otimes
    |\chi_{1}\rangle$
    (z pominięciem normalizacji).

    W przypadku stanów mieszanych, wyrażanych poprzez macierze gęstości, definicje te
    należy zmodyfikować. Gdy mówimy o stanie splątanym, mamy na myśli taki którego nie
    da się wyrazić jako mieszaninę statystyczną czystych stanów produktowych, natomiast stan
    separowalny, daje się wyrazić w ten sposób.

    Po wczytaniu $\rho_{0}$ i stworzeniu $\rho_{1}$ program postępuje zgodnie z
    następującymi krokami:

    \begin{enumerate}
      \item Zwiększ licznik prób $c_{t}$ o 1. Wylosuj czysty stan produktowy $\rho_{2}$,
        zwany dalej stanem próbnym.

      \item Uruchom preselekcję dla stanu próbnego poprzez sprawdzenie funkcji liniowej.
        Jeśli się nie powiedzie, wróć do punktu 1.

      \item W przypadku udanej preselekcji symetryzujemy $\rho_{1}$ względem wszystkich symetrii
        przez $\rho_{0}$, które respektują separowalność.

      \item Znaleźć minimum $Tr(\rho_{0}- p\rho_{1}- (1 - p)\rho_{2})^{2}$ względem p.

      \item Jeśli minimum występuje dla $0 \le p \le 1$, zaktualizuj
        $\rho_{1}\leftarrow p\rho_{1}- (1 - p)\rho_{2}$, dodać nową wartość $D^{2}(\rho_{0}
        , \rho_{1})$ do listy listy i zwiększyć wartość licznika sukcesu $c_{s}$ o 1.

      \item Przejdź do kroku 1, aż spełnione zostanie wybrane kryterium zatrzymania.
    \end{enumerate}

    Jako dane wyjściowe program zapisuje historię poprawek i stan $\rho_{1}$ do plików.
    Dostępnymi kryteriami zatrzymania jest maksymalna liczba korekcji do uzyskania oraz
    maksymalna liczba iteracji do wykonania - ta z tych wartości, która zostanie osiągnięta
    jako pierwsza decyduje o zatrzymaniu programu. Jeśli wyznaczona przez program
    odległość HSD jest odpowiednio niewielka (tj. mniejsza niż $1 \cdot 10^{-4}$) oznacza
    to, że stan jest praktycznie separowalny. W przeciwnym wypadku stan można uznać za splątany.

    \subsection{Cel pracy}
    Celem tej pracy jest eksploracja dostępnych metod maksymalizacji wydajności programu
    CSSFinder. Zakłada on:
    \begin{enumerate}
      \item określenie które fragmenty kodu programu są kluczowe dla jego wydajności,

      \item zidentyfikowanie dostępnych metod (narzędzi) pozwalających na optymalizację czasu
        wykonania programu,

      \item wstępną selekcję metod ocenianych jako potencjalnie najbardziej skuteczne,

      \item dokonanie odpowiednich modyfikacji w programie, w tym pełnej re-implementacji
        wszystkich kluczowych dla wydajności elementów programu z wykorzystaniem kolejno
        każdej z metod,

      \item weryfikację uzyskanej poprawy wydajności poprzez przeprowadzenie pomiarów czasu
        pracy dla tych samych danych wejściowych w przypadku różnych wariantów kodu,

      \item podsumowanie, które z metod okazały się najbardziej skuteczne.
    \end{enumerate}

    \subsection{Przyczyny przystąpienia do optymalizacji}
    Podczas analizy przestrzeni stanów kwantowych często konieczne jest przeanalizowanie
    wielu stanów. Wymaga to wielokrotnego wywoływania programu CSSFinder dla wielu różnych
    macierzy wejściowych. Dlatego też preferowanym jest aby obliczenia dla jednego stanu
    trwały jak najkrócej.

    Niestety, język Python wykorzystany do stworzenia oryginalnej implementacji jest powszechnie
    znany z problemów z wydajnością\cite{srinath2017python}. Są one pokłosiem faktu, że
    jest to interpretowany język programowania, a więc konieczne jest by specjalny program
    (tak zwany interpreter) wykonywał instrukcje zawarte w kodzie programu. Dodatkowo
    jest to język dynamicznie typowany z bardzo rozbudowanymi możliwościami introspekcji.
    Uniemożliwia to zastosowanie wielu z optymalizacji powszechnie wykorzystywanych w innych
    językach programowania. Warto jednak mieć na uwadze, że cechy te są jednocześnie jednymi
    z największych zalet Pythona, więc nie bez powodu pozostają częścią języka.

    Aby zwiększyć wydajność, koniecznym jest więc poczynić pewne kompromisy i zrezygnować
    z rozwiązań wygodnych na rzecz rozwiązań bardziej optymalnych dla wydajności.
    Jednocześnie niekorzystnym byłoby zacząć od podejmowania zbyt radykalnych kroków, na
    przykład od razu sięgać po język asemblera, który wymaga stworzenia dużej ilości
    skomplikowanego kodu, jeśli języki wyższego poziomu mogą zaoferować podobne osiągi.
    Z tego względu w dalszej części pracy rozważę kilka rozwiązań które czynią mniej
    radykalne kompromisy i wymagają różnej ilości dodatkowego wysiłku aby uzyskać
    sprawny program.

    \section{Narzędzia}
    \subsection{Kompilacja AOT}
    Kompilacja AOT (Ahead Of Time) to proces tłumaczenia jednej reprezentacji programu (na
    przykład w języku programowania wysokiego poziomu) na inną (na przykład kod maszynowy)
    przed rozpoczęciem pracy kompilowanego programu.

    Obecnie najpowszechniej używana implementacja języka Python, CPython, posiada
    możliwość korzystania z bibliotek współdzielonych (.so - Linux, .dll/.pyd - Windows)
    które powstały w skutek kompilacji kodu wysokiego poziomu. Dostęp do funkcji zawartych
    w takich bibliotekach można uzyskać na kilka sposobów:

    \begin{enumerate}
      \item Przy pomocy API modułu ctypes\cite{Python_ctypes}. Pozwala ono opisać interfejs
        funkcji obcej (tj. napisana w języku niższego poziomu i skompilowana do kodu maszynowego)
        i wywołać tak opisaną funkcję.

      \item Poprzez zawarcie w bibliotece odpowiednio nazwanych symboli, automatycznie
        rozpoznawanych przez interpreter języka Python. Takie biblioteki określa się mianem
        modułów rozszerzeń \cite{Extending_Python_With_C_Cpp}. W tym przypadku warto
        dodać, że oficjalna dokumentacja\cite{Python_extension} wspomina tylko o
        językach C i C++, natomiast powstały biblioteki które pozwalają wykorzystać w łatwy
        sposób wiele innych języków programowania, takich jak Rust\cite{Rust_Programming_Language}
        przy pomocy, na przykład, biblioteki Py03\cite{PyO3} lub GO z użyciem biblioteki
        gopy\cite{gopy}.

      \item Wykorzystując bibliotekę Cython\cite{Cython_Org}\cite{Cython_The_Best_Of_Both}.
        Oferuje ona dedykowany język, o tej samej nazwie, który jest nadzbiorem języka
        Python, który rozszerza jego składnię o możliwość statycznego typowania.
        Biblioteka zawiera transpilator, zdolny przetłumaczyć dedykowany język na C/C++,
        a następnie, wykorzystując osobno zainstalowany kompilator, skompilować do kodu
        maszynowego.

      \item Kompilując kod pythona z użyciem biblioteki mypyc\cite{mypyc}. Ta, podobnie
        do biblioteki Cython, również zawiera transpilator, natomiast zamiast korzystać
        z dedykowanego języka, opiera się on na dodanych w Pythonie 3.5\cite{Python_3_5}
        (PEP 484\cite{PEP_484} i PEP 483\cite{PEP_483}), adnotacjach typów. Jest on
        rozwijany obok projektu mypy - pakietu do statycznej analizy typów dla języka
        Python, również opartej na adnotacjach typów\cite{mypy}.
    \end{enumerate}

    Ponieważ w każdym z wymienionych przypadków, kod niższego poziomu jest kompilowany przed
    dostarczeniem do użytkownika, pozwala to na wykorzystanie zaawansowanych możliwości
    automatycznej optymalizacji dostarczanych przez współczesne kompilatory, na przykład
    LLVM, które jest sercem implementacji clang\cite{ClangHomePage} (język C++) oraz rustc
    (język Rust).

    \subsection{Kompilacja JIT}
    Kompilacja JIT to proces tłumaczenia jednej reprezentacji programu (na przykład w
    języku programowania wysokiego poziomu) na inną (na przykład kod maszynowy) po rozpoczęciu
    pracy programu. Zazwyczaj wymaga to aby program rozpoczynał pracę w trybie
    interpretowanym, a następne kompilował sam siebie i przechodził w tryb wykonywania skompilowanego
    kodu.

    W momencie pisania tej pracy istnieją dwa szeroko dostępne i aktywnie utrzymywane narzędzia
    oferujące kompilację JIT dla języka Python.

    Pierwszym z nich jest pełna alternatywna implementacja języka Python - PyPy\cite{PyPy_Home_Page}.
    Wykonywana przez nią kompilacja JIT śledzi cały kod który wykonuje i automatycznie decyduje
    które fragmenty skompilować do kodu maszynowego\cite{PyPy_JIT}\cite{PyPy_JIT_Compiler}.
    Jednocześnie posiada odmienny interfejs binarny, więc pakiety wheel dla implementacji
    CPython nie są z nią kompatybilne i muszą być rekompilowane.

    Drugim narzędziem jest biblioteka Numba\cite{Numba_Article}\cite{Numba_Doc}. Ona, w przeciwieństwie
    do PyPy, wymaga aby fragmenty kodu, które mają być skompilowane, miały postać
    funkcji oznaczonych dedykowanymi dekoratorami. Może dokonywać kompilacji w kilku trybach.
    Najbardziej interesujący jest tryb który usuwa całkowicie dynamiczność języka Python
    i tworzy kod całkowicie statyczny, ponieważ może on oferować największy wzrost wydajności.

    \subsection{Selekcja narzędzi}
    \FloatBarrier
    \begin{table}[ht]
      \centering
      \input{resources/tools/tools.tex}
      \caption{Wybrane narzędzia.}
      \label{selected-tool}
    \end{table}
    \FloatBarrier

    Tablica \ref{selected-tool} zawiera zestawienie języków programowania i
    zastosowanych bibliotek użytych do wykonania re-implementacji algorytmu CSSF.

    \subsubsection{Python i NumPy}
    Pierwszą wykonaną przeze mnie re-implementacją algorytmu, napisałem w języku Python,
    a do realizowania obliczeń na macierzach liczb zespolonych wykorzystywałem
    bibliotekę NumPy. Był to dokładnie taki same zestaw, jak wykorzystany do oryginalnej
    implementacji. Podczas przepisywania podjąłem jednak dodatkowe wysiłki aby zastępować
    kod Pythona wywołaniami do funkcji zawartych w bibliotece NumPy. Ponieważ kluczowe
    dla wydajności fragmenty kodu tego pakietu są zaimplementowane w języku niższego
    poziomu, a następnie skompilowane kompilatorem optymalizującym, oferują znacznie
    wyższą wydajność niż analogiczny kod napisany w języku Python. Proces ten pozwolił mi
    również zapoznać się lepiej z charakterystyką programu i zmodyfikować interfejs służący
    do komunikacji pomiędzy częścią główną, a samą implementacją (backend'em).

    \subsubsection{Python i NumPy z AOT}
    Następnym wykonanym przeze mnie krokiem było skompilowanie mojej implementacji
    korzystającej z NumPy do kodu maszynowego przy pomocy biblioteki Cython. Kod przeznaczony
    do takiej kompilacji nie musi być adnotowany dedykowanymi informacjami o typach.
    Zostanie on wtedy przetłumaczony na odpowiednie operacje w języku C/C++, a potem skompilowany
    do kodu maszynowego. Brak adnotacji powoduje niestety, że program zachowuje swoją dynamiczną
    naturę, charakterystyczną dla języka Python. Kompilacja pozwala jednak usunąć dodatkowy
    narzut na procesor ze strony interpretera. W takim scenariuszu spodziewać należy się,
    że zyski z kompilacji będą niewielkie, ale mogą wystąpić.

    \subsubsection{Python i NumPy z JIT}
    Tworząc ostatnią re-implementację w języku Python, opierającą się na bibliotece NumPy,
    dodatkowo skorzystałem z kompilacji JIT. Pakiet Numba, który oferuje możliwość
    kompilacji JIT kodu Pythona, posiada dwa tryby pracy. Pierwszy wykonuje kompilację
    na podstawie specjalnie dostarczonych przez programistę deklaracji typów dla funkcji
    podlegających kompilacji i jest wykonywany zaraz po rozpoczęciu pracy programu\footnote{ang.
    eager (compilation) - niecierpliwa (kompilacja).}. Drugi polega na śledzeniu typów wejściowych
    i wyjściowych funkcji i automatycznie kompiluje funkcję dla tych typów danych które są
    odpowiednio często używane\footnote{ang. lazy (compilation) - leniwa (kompilacja).}.

    Ponadto, Numba posiada dodatkowe parametry kompilacji, które można przekazać do funkcji
    \code{numba.jit}. Jednym z nich, posiadającym szczególnie duży wpływ na wydajność, flaga
    \code{nopython}. Tryb \code{nopython=True} oferuje znacznie większe możliwości
    optymalizacji i potencjalnie lepszą wydajność. Niestety nie wszystkie funkcje dostępne
    w bibliotece NumPy są akceptowane przez kompilator JIT pakietu Numba w trybie \code{nopython=True}.
    Do niekompatybilnych należy między innymi funkcja tensordot która implementuje mnożenie
    tensorowe. Jest ona używana w kodzie programu podczas procesu tworzenia losowych
    macierzy unitarnych w funkcji \code{kronecker()} (Patrz rysunek \ref{pre-prof-perf},
    pierwsza kolumna od lewej, drugi i trzeci wiersz od dołu). Funkcja ta może zostać
    skompilowana tylko w trybie obiektowym (\code{nopython=False}), który po kompilacji
    zachowuje dynamiczną naturę Pythona. Niestety, brak możliwości skompilowania funkcji
    używającej tensordot powoduje również brak możliwości skompilowania funkcji wyżej w
    drzewie wywołań. W efekcie znacząca część implementacji używającej JIT musi używać trybu
    obiektowego.

    \subsubsection{Rust i Ndarray}
    Aby uczynić to porównanie jak najpełniejszym, podjąłem również wysiłek
    zaimplementowania części obliczeniowej programu w języku Rust.

    Język ten wybrałem z kilku względów. Przede wszystkim posiada on gotową, rozbudowaną
    infrastrukturę narzędzi pomocniczych. Do tych narzędzi zaliczyć należy menadżera pakietów
    cargo, który pozwala zarówno w łatwy sposób kompilować bardziej rozbudowane projekty
    i tworzyć z nich łatwe do obsługi pakiety, jak również daje możliwość korzystania z pakietów
    udostępnionych przez innych programistów (w tym projekcie wykorzystany został między
    innymi pakiet Ndarray\cite{Ndarray}).

    Pozwoliło to w łatwy i szybki sposób skompletować zestaw bibliotek umożliwiających
    wydajne i wygodne tworzenie kodu implementacji algorytmu CSSF. Ponadto istnienie biblioteki
    PyO3 znacząco uprościło proces tworzenia interfejsu pozwalającemu interpreterowi języka
    Python na interakcję z tą implementacją.

    \newpage


    Jednocześnie, język Rust jest językiem:
    \begin{enumerate}
      \item kompilowanym,

      \item wykorzystującym zestaw narzędzi kompilatora LLVM,

      \item statycznie typowanym,

      \item posiadającym automatyczny system zarządzania pamięcią oparty na koncepcji posiadania
        (ang. ownership), który usuwa konieczność manualnego zarządzania pamięcią,
        zarazem bez konieczności wprowadzania mechanizmu liczenia referencji i
        dedykowanego automatycznego `odśmiecacza' (ang. garbage collector).
    \end{enumerate}

    Cechy te pozwalają oczekiwać, że skompilowany kod będzie osiągał wydajność zbliżoną do
    kodu C/C++, skompilowanych przy pomocy kompilatora clang, który również wykorzystuje
    LLVM do optymalizacji kodu.

    Cały proces wstępnej konfiguracji sprowadził się do około godziny, co stanowi
    wyśmienity wynik, a cały proces implementacji zajął niewiele więcej czasu niż implementacja
    w języku Python. Jednocześnie język Rust posiada system typów który jest w stanie
    pomieścić bardzo dużo informacji o zamiarach programisty. W efekcie kompilator ma możliwość
    wychwycić wiele błędów, których nie może zauważyć kompilator języka C++.

    \subsubsection{Rust i Ndarray z OpenBLAS}
    Biblioteka Ndarray, która jest sercem implementacji w języku Rust, posiada
    przełącznik funkcjonalności\footnote{ang. feature switch} który pozwala wykorzystać funkcje
    zawarte w bibliotece OpenBLAS\cite{OpenBLAS} jako implementację mnożenia
    macierzowego. Powoduje to niestety, że kompilacja programu zaczyna wymagać by
    biblioteka OpenBLAS była zainstalowana i dostępna podczas kompilacji, co jest trudne
    do uzyskania w środowisku które wykorzystuję do kompilacji. W efekcie kompilacja dla
    wszystkich platform które ma wspierać CSSFinder (Windows, Linux i MacOS) stanowi wyzwanie,
    ale jest możliwa. Dlatego też w zestawieniu wziąłem pod uwagę również pod uwagę tę implementację.

    \section{Metody}
    \subsection{Modularyzacja}
    Re-implementując program CSSFinder planowałem wypróbować liczne rozwiązania, które
    wymagały zasadniczych zmian w kodzie algorytmu, w tym przepisania go w innym języku programowania.
    Jednocześnie część programu odpowiadająca za interakcję z użytkownikiem i ładowanie
    zasobów miała pozostawać taka sama. Zdecydowałem więc, że tworzony przeze mnie kod
    musi być modularny, aby uniknąć duplikacji wspólnych elementów. Tak też program
    podzieliłem na dwie części: główną (`core'), z interfejsem użytkowników i
    narzędziami pomocniczymi oraz część implementującą algorytm (`backend'). Korpus w całości
    napisałem w języku Python i wykorzystałem wbudowany w ten język mechanizm importowania
    bibliotek w celu wykrywania i ładowania dostępnych implementacji algorytmu (`backendów').
    Dane macierzowe w obrębie korpusu przechowywane są jako obiekty Ndarray z biblioteki
    NumPy, ze względu na uniwersalność w świecie bibliotek do obliczeń tensorowych (wiele
    bibliotek w innych językach programowania oferuje gotowe narzędzia do transformacji
    obiektów Ndarray na reprezentacje charakterystyczne dla tych bibliotek).

    Pozwala to na proste podmiany implementacji o dowolnie różnym pochodzeniu, w tym
    implementacje w językach kompilowanych. Uprościło to znacznie proces weryfikacji zmian
    w zachowaniu programu i przyspieszyło proces tworzenia kolejnych implementacji, jako
    że kod interfejsu programistycznego jest mniej pracochłonny niż kod pozwalający na
    interakcję z użytkownikiem. W przyszłości może to również pozwolić na łatwiejszy rozwój
    nowych implementacji oraz dodawanie nowych funkcjonalności do programu.

    \subsection{Dane testowe}
    Podczas pomiarów konsekwentnie wykorzystywałem ten sam zestaw macierzy gęstości, aby
    móc wygodnie porównywać wyniki wydajności poszczególnych implementacji. W dalszej części
    pracy będę wielokrotnie odnosił się do tych macierzy posługując się symbolem $\rho$ z
    liczbą w indeksie dolnym. Liczba ta będzie wskazywać na konkretną z wymienionych
    poniżej macierzy.

    \FloatBarrier
    \begin{figure}[ht]
      \centering
      \setcounter{MaxMatrixCols}{33}
      \input{resources/matrices/rho1.tex}
      \caption{Macierz $\rho_{1}$.}
      \label{rho-1}
    \end{figure}

    \FloatBarrier

    Pierwsza macierz (rysunek \ref{rho-1}) opisuje układ 5 kubitów i ma wymiary
    $32\times 32$. Pomimo że nie zawiera ona wartości, podczas analizy zawsze będzie
    reprezentowana przez macierze zawierające liczby zespolone, ponieważ szczególnie kosztowne
    obliczeniowo części algorytmu wymagają, aby części urojone były obecne, co znaczy,
    że usuwanie ich w wybranych miejscach nie niesie wymiernych zysków wydajnościowych.

    Następnie w zbiorze macierzy wykorzystywanych jako dane wejściowe znajduje się pięć
    macierzy reprezentujących układy od 2 do 6 kubitów, które przyjmują rozmiary od $4\times
    4$ do $64\times64$. Są one wypełnione zerami poza pierwszym i ostatnim elementem w
    pierwszej i ostatniej kolumnie - te przyjmują wartość $0.5$.

    \FloatBarrier
    \begin{figure}[ht]
      \centering
      \setcounter{MaxMatrixCols}{33}
      \[
        \rho_{n}=
        \begin{bmatrix}
          0.5    & 0      & \hdots & 0      & 0.5    \\
          0      & 0      & \hdots & 0      & 0      \\
          \vdots & \vdots &        & \vdots & \vdots \\
          0      & 0      & \hdots & 0      & 0      \\
          0.5    & 0      & \hdots & 0      & 0.5    \\
        \end{bmatrix}_{(2^{n}\times2^{n})}
      \]
      \caption{Ogólna postać macierzy $\rho_{2}- \rho_{6}$.}
      \label{rho-2-6}
    \end{figure}

    \FloatBarrier

    W tekście macierze te będą oznaczane jako $\rho_{2}$ do $\rho_{6}$, w zależności od reprezentowanej
    liczby kubitów\footnote{Tak więc macierz $\rho_{2}$ ma wymiary $4\times 4$ i
    reprezentuje 2 kubity, macierz $\rho_{3}$ ma wymiary $8\times8$ i reprezentuje 3 kubity,
    macierz $\rho_{4}$ ma wymiary $16\times16$ i reprezentuje 4 kubity, itd. aż do
    $\rho_{6}$, $64\times64$ .}.

    \subsection{Środowisko testowe}
    Podczas pomiarów wydajności wykorzystywałem każdorazowo to samo środowisko testowe. Do
    chłodzenia CPU wykorzystywane było chłodzenie wodne typu AIO, temperatura w pokoju
    oscylowała w okolicy 25°C, procesor podczas testów wydajności nie doświadczał temperatur
    powyżej 80°C.

    \FloatBarrier
    \begin{table}[ht]
      \centering
      \input{resources/pc/pc.tex}
      \caption{Konfiguracja środowiska testowego.}
      \label{pc-configuration}
    \end{table}
    \FloatBarrier

    \subsection{Profilowanie}
    Podczas prac nad optymalizacją czasu pracy programu kluczowym było zbieranie
    informacji na temat tego, które fragmenty kodu są kluczowe dla wydajności całego programu.
    Rzadko bowiem zdarza się, by wszystkie operacje wykonywane przez program miały równomierny
    wkład w czas wykonania. Standardowo proces zbierania takich danych określa się
    mianem profilowania. Technologie, po które sięgałem podczas re-implementacji
    algorytmu posiadają gotowe narzędzia pozwalające na skuteczne pozyskiwanie takich
    danych oraz ich wizualizację.

    Dla kodu w języku Python, implementacja CPython tego języka posiada w bibliotece standardowej
    dwa dedykowane moduły oferujące funkcjonalność profilowania: `profile' i `cProfile'.
    Pierwszy jest zaimplementowany w języku Python, drugi w C. Ponieważ drugi z nich posiada
    mniejszy dodatkowy narzut na procesor, zdecydowałem żeby to na nim oprzeć moje
    analizy. W celu wizualizacji uzyskanych wyników posłużyłem się otwartoźródłowym programem
    Snakeviz\cite{Snakeviz_PyPI}.

    Do zbierania informacji na temat charakterystyki pracy kodu napisanego w języku Rust
    wykorzystałem narzędzie perf pochodzące z pakiety linux-tools-5.19.0-42-generic pobranego
    przy pomocy menadżera pakietów apt-get. Do wizualizacji uzyskanych wyników
    wykorzystałem jedno z otwartoźródłowych narzędzi funkcjonujące pod nazwą hotspot\cite{HOTSPOT}.

    \subsection{Precyzja obliczeń}
    Oryginalny program, jak i pierwsze stworzone przeze mnie re-implementacje
    posługiwały się liczbami zespolonymi składającymi się z liczb zmiennoprzecinkowych
    podwójnej precyzji. Jedna taka liczba zajmuje 64 bity. Jednak w wielu przypadkach
    taka precyzja obliczeń nie jest konieczna do uzyskania poprawnych wyników. Podstawową
    zaletą wykorzystania liczb zmiennoprzecinkowych pojedynczej precyzji, czyli 32
    bitowych, jest zmniejszenie rozmiaru macierzy. Pozwala na umieszczenie większej
    części macierzy w pamięci podręcznej procesora. Dodatkowo zwiększa to przepustowość obliczeń
    wykorzystujących instrukcje SIMD, ponieważ wykorzystują one rejestry o stałych
    rozmiarach (128, 256, 512 bitów) które mogą na ogół pomieścić dwukrotnie więcej
    liczb 32 bitowych niż 64 bitowych. Pozwala to oczekiwać, że obliczenia
    wykorzystujące liczby zmiennoprzecinkowe pojedynczej precyzji będą trwały krócej.

    Tworzony przeze mnie kod od początku powstawał z zamysłem umożliwienia wykorzystania
    liczb zmiennoprzecinkowych o różnych precyzjach, dlatego transformacja ta była dość prosta.
    W języku Python, wykorzystując bibliotekę NumPy przejście na liczby pojedynczej precyzji
    wymagało prawie każdorazowego deklarowania, że wynik operacji ma posiadać typ
    complex64 (cały czas mówimy o liczbach zespolonych, które składają się z dwóch
    wartości zmiennoprzecinkowych). Nie wszystkie operacje, które przyjmują parametr określający
    typ wejściowy są akceptowane przez kompilator JIT biblioteki Numba, gdy jest on
    przekazywany. To ograniczenie można obejść, wykonując zmianę typu, jako osobną operację,
    przy pomocy metody \code{astype()}.

    Warto tutaj zaznaczyć, że wszystkie implementacje w języku Python powstają ze
    wspólnego szablony, który był ewaluowany przez bibliotekę Jinja2 do różnych wariantów
    kodu, w zależności od tego jakie parametry były do niego przekazywane. Pozwoliło to uniknąć
    wielokrotnego pisania wspólnych fragmentów kodu, a elementy unikalne są dodawane
    warunkowo. Zastosowanie introspekcji do konstruowania odpowiedniego kodu w trakcie wykonywania
    programu, mogłoby w znaczący sposób obniżyć wydajność, dlatego zdecydowałem się sięgnąć
    po system bardziej statyczny, który na pewno nie wpływał na czas pracy programu.

    W przypadku języka Rust, posiada on dedykowany konstrukt składniowy pozwalający na deklarowanie
    funkcji w oparciu o symbole zastępcze wobec których stawia się zbiór wymagań dotyczących
    wspieranych interfejsów. W efekcie funkcja-szablon może zostać wyspecjalizowana żeby
    akceptować zarówno liczby zespolone skonstruowane z liczb zmiennoprzecinkowych
    pojedynczej, jak i podwójnej precyzji. Pozwoliło to uniknąć sięgania po zewnętrzne
    mechanizmy do tworzenia szablonów, tak jak było to konieczne w języku Python.

    \subsection{Wykresy}
    Wszystkie wykresy zamieszczone w tej pracy zostały utworzone przy pomocy skryptów w języku
    Python z wykorzystaniem biblioteki matplotlib\cite{Hunter:2007}.

    \section{Wyniki}
    \FloatBarrier
    \subsection{Wstępne profilowanie}
    Prace nad optymalizacją kodu rozpocząłem od wstępnego profilowania pracy programu w
    trybie 1 (ang. full separability of an n-quDit state) przekazując do obliczeń układ 5
    kubitów opisany macierzą $\rho_{1}$ (Rysunek \ref{rho-1}).

    Program wykonywał proces analizy stanu aż do uzyskania 1000 korekcji. Przekazany
    limit liczby iteracji wynosił 2.000.000 i nie został osiągnięty. Podczas pomiarów, program
    wykorzystywał domyślny globalny generator liczb losowych biblioteki NumPy (PCG64\cite{NumpyDefaultGenerator})
    z ziarnem ustawionym na wartość 0.

    \begin{figure}[ht]
      \centering
      \includegraphics[width=1.0\textwidth]{"resources/profiling_1/graph.png"}
      \caption{Diagram typu Icircle wizualizujący charakterystykę pracę programu wygenerowany przez program Snakeviz na podstawie danych profilowania zabranych przez moduł cProfile.}
      \label{pre-prof-perf}
    \end{figure}

    Pozwoliło mi to wstępnie przyjrzeć się charakterystyce pracy programu i ocenić czy
    powszechnie dostępne narzędzia mogą zostać wykorzystane w tym wypadku. Rysunek \ref{pre-prof-perf}
    przedstawia diagram typu Icicle obrazujący udział czasu pochłoniętego przez wykonywanie
    poszczególnych funkcji, w całkowitym czasie pracy programu. Pierwszy blok od góry (\code{~:0(<built-in method builtins.exec>)})
    to wywołanie funkcji wykonującej kod programu. Następne bloki idąc w dół wykresu, to
    kolejne warstwy wywołań funkcji. Te których opisy zaczynają się od `CSSFinder.py' to
    wywołania w kodzie programu. Bloki umieszczone najniżej, w większości pozbawione opisów,
    to wywołania do funkcji bibliotek, głównie NumPy, ale również modułów wbudowanych
    Pythona. Snakeviz automatycznie podejmuje decyzję o nie adnotowaniu bloku gdy opis nie
    ma szansy zmieścić się w obrębie bloku. Aby usunąć z diagramu zbędny szum
    informacyjny, funkcje których wykonywanie zajęło mniej niż 1\% czasu programu były
    pomijane.

    \begin{table}[ht]
      \tiny
      \centering
      \input{resources/profiling_1/profiling.tex}
      \caption{Dane dotyczące pracy oryginalnej implementacji programu CSSFinder uzyskane przy pomocy programy cProfile. Tablica posiada oryginalne nazwy kolumn, nadane przez program Snakeviz. Znaczenia kolumn, kolejno od lewej: \code{ncalls} - liczba wywołań funkcji. \code{tottime} - całkowity czas spędzony w ciele funkcji bez czasu spędzonego w wywołaniach do podfunkcji. \code{percall} - \code{totime} dzielone przez \code{ncalls}. \code{cumtime} - całkowity czas spędzony wewnątrz funkcji i w wywołaniach podfunkcji. \code{percall} - \code{cumtime} dzielone przez \code{ncalls}. \code{filename:lineno(function)} - Plik, linia i nazwa funkcji.}
    \end{table}

    Z uzyskanych danych wynika, że znakomitą większość (77\%\footnote{Wartość 77\% jak i
    wartości procentowe dalszej części tego akapitu zostały zaokrąglone do jedności, ze względu
    na małe znaczenie rzeczowe części ułamkowych.}) czasu pracy programu zajmuje funkcja
    \code{OptimizedFS()}. W jej wnętrzu 38\% czasu pochłania proces generowania losowych
    macierzy unitarnych, który w dużej mierze wykorzystuje mnożenia tensorowe (26\%).
    Poza funkcją \code{OptimizedFS()}, znaczący wpływ na czas wykonywania ma też funkcja
    `rotate()`, która pochłania około 21\% czasu działania programu. Kolejne 20\% czasu
    zajmuje funkcja \code{product()}, obliczająca odległość Hilberta-Schmidta pomiędzy
    dwoma stanami. Pozostałe wywołania mają stosunkowo marginalny wpływ na czas pracy i ich
    analiza na tym etapie nie niesie za sobą znaczących korzyści.

    Takie wyniki wskazują jednoznacznie, że kluczowa dla czasu pracy programu jest tu
    maksymalizacja wydajności pętli optymalizacyjnej, w tym zawartych w niej operacji macierzowych.
    Najprostszym sposobem na na uzyskanie takich efektów jest zastąpienie dynamicznego
    systemu typów i kodu bajtowego algorytmu wykonywanego przez interpreter pythona na
    statyczny system typów i kod maszynowy. Dodatkowo, niezastąpione są biblioteki
    zawierające wyspecjalizowane implementacje operacji macierzowych, takie jak OpenBLAS.
    Profilowanie pozwoliło również wykluczyć problemy z operacjami zapisu/odczytu plików
    oraz inne niespodziewane zjawiska.

    \subsection{Wstępne pomiary wydajności}
    Aby uzyskać dobrą bazę porównawczą, wykonałem serię pomiarów czasu pracy programu na
    macierzach $\rho_{1}$, $\rho_{2}$ - $\rho_{6}$, przedstawionych na rysunkach \ref{rho-1}
    i \ref{rho-2-6}.

    Dane przekazywałem kolejno do programu z poleceniem działania w trybie 1 (full
    separability of an n-quDit state) do osiągnięcia 1000 korekcji lub do 2.000.000
    iteracji algorytmu, w zależności od tego co nastąpi szybciej. Dla wszystkich
    macierzy algorytm uzyskał 1000 korekcji i w żadnym przypadku nie osiągnął
    maksymalnej liczby iteracji. Dla każdej macierzy pomiar był powtarzany pięciokrotnie,
    a wyniki z pomiarów zostały uśrednione. Podczas obliczeń ziarno globalnego generatora
    liczb losowych biblioteki NumPy było ustawione na 0. Pomiary czasu pracy dotyczyły wyłącznie
    samego algorytmu\footnote{tj. funkcji `Gilbert()', nie biorą więc pod uwagę czasu
    pochłoniętego przez importowanie modułów, ładowanie danych itp. natomiast operacje
    pisania do plików które były wykonywane w obrębie tej funkcji są wliczane w czas
    pracy.}.

    \begin{figure}[ht]
      \centering
      \includegraphics[width=1.0\textwidth]{"resources/original_performance_tests.png"}
      \caption{Wyniki wstępnych testów wydajności oryginalnego kodu dla macierzy $\rho_{1}$ - $\rho
      _{6}$.}
      \label{pre-perf}
    \end{figure}

    Podczas testów zaobserwowałem interesujące zjawisko dotyczące wydajności dla macierzy
    $64\times64$. W przypadku takich rozmiarów danych biblioteka NumPy automatycznie
    decyduje o wykorzystaniu wielowątkowej implementacji mnożenia macierzowego. Niestety,
    daje to efekt odwrotny do zamierzonego - obliczenia zamiast przyspieszać zwalniają. Na
    rysunku \ref{pre-perf} zostały przedstawione czasy obliczeń dla macierzy $\rho_{1}$ -
    $\rho_{6}$ z domyślnym zachowaniem biblioteki.

    \begin{figure}[ht]
      \centering
      \includegraphics
      [width=1.0\textwidth]{"resources/original_performance_tests_locked.png"}
      \caption{Wyniki wstępnych testów wydajności oryginalnego kodu z zablokowaną liczbą wątków obliczeniowych dla macierzy $\rho
      _{1}$ - $\rho_{6}$.}
      \label{pre-perf-locked}
    \end{figure}

    Jeśli przy pomocy zmiennych środowiskowych ustawimy liczbę wątków wykorzystywanych
    do obliczeń na 1 uzyskujemy znaczące skrócenie czasu obliczeń dla macierzy
    $64\times64$. Wyniki testów w takich warunkach zostały przedstawione na rysunku \ref{pre-perf-locked}.

    Aby ułatwić porównywanie czasu pracy różnych wariantów programu, na tym i następnych
    wykresach w sekcjach \ref{plots-double-precision} i \ref{plots-single-precision}
    pozostawiam taką samą skalę na osi Y.

    Dla macierzy o mniejszych rozmiarach niż $64\times64$ nie odnotowałem różnicy w
    wydajności pomiędzy konfiguracją domyślną, a manualnie dostosowywaną. Warto dodać że
    liczba iteracji wykonywanych przez program nie zmienia się, różnica wynika wyłącznie
    z czasu trwania operacji arytmetycznych. Taki stan rzeczy najprawdopodobniej jest
    wynikiem dodatkowego obciążenia ze strony komunikacji i/lub synchronizacji między
    wątkami.

    Chciałbym uściślić, że w dalszej części pracy, mówiąc o wynikach oryginalnego kodu, będę
    miał na myśli wersję bez zablokowanej liczby wątków, a więc tę której wyniki
    umieszczone są na rysunku \ref{pre-perf}, jako, że to była pierwotna postać kodu, natomiast
    zablokowanie liczby wątków wymagało już jego modyfikacji.

    \subsection{Pomiary z podwójną precyzją}
    \label{plots-double-precision} W dalszej części pracy prezentuje wyniki pomiarów czasu
    pracy re-implementacji algorytmu CSSF wykorzystujących liczby zmiennoprzecinkowe podwójnej
    precyzji.

    \subsubsection{ Python i NumPy }
    Pomiary czasu pracy były wykonywane przy użyciu macierzy $\rho_{1}$ - $\rho_{6}$.
    Dane przekazywałem kolejno do programu z poleceniem działania w trybie FSnQd\footnote{Tryb
    FSnQd jest odpowiednikiem trybu 1 (full separability of an n-quDit state) z oryginalnego
    kodu.} do osiągnięcia co najmniej 1000 korekcji lub do 2.000.000 iteracji algorytmu,
    w zależności od tego co nastąpi szybciej. Dla wszystkich macierzy algorytm uzyskał
    co najmniej 1000 korekcji i w żadnym przypadku nie osiągnął maksymalnej liczby
    iteracji. Dla każdej macierzy pomiar powtarzałem pięciokrotnie, a wyniki uśredniłem.
    Na potrzeby obliczeń ziarno domyślnego globalnego generatora liczb losowych biblioteki
    NumPy ustawiłem na 0. Program działał z zablokowaną liczbą wątków obliczeniowych.
    Pomiary czasu pracy dotyczyły przede wszystkim samego algorytmu\footnote{Pomiary nie
    biorą więc pod uwagę czasu pochłoniętego przez importowanie modułów itp., natomiast operacje
    wczytywania danych i pisania do plików są wliczane w czas pracy, ponieważ wbudowany
    w program mechanizm pomiaru czasu pracy rozpoczyna pomiar zanim dane zostaną
    załadowane.}.

    \begin{figure}[!ht]
      \centering
      \includegraphics
      [width=1.0\textwidth]{"resources/python_and_numpy_performance_tests.png"}
      \caption{Wyniki testów wydajności alternatywnej implementacji Python z użyciem biblioteki NumPy dla macierzy $\rho
      _{1}$ - $\rho_{6}$.}
      \label{python-numpy-double-precision}
    \end{figure}

    Uzyskane wyniki zostały przedstawione na rysunku \ref{python-numpy-double-precision}.
    W przypadku małych macierzy wyniki są bardzo zbliżone, natomiast w przypadku macierzy
    $32\times32$ i $64\times64$ występuje znacząca poprawa wydajności, odpowiednio
    $7.9s$ ($\approx 21\%$) dla $\rho_{1}$, $12.7s$ ($\approx 27\%$) dla $\rho_{5}$ i $20
    5.2s$ ($\approx 50\%$) dla $\rho_{6}$.

    \subsubsection{ Python i NumPy z AOT }
    Pomiary czasu pracy były wykonywane w taki sam sposób jak dla implementacji bez AOT.

    \begin{figure}[!ht]
      \centering
      \includegraphics
      [width=1.0\textwidth]{"resources/python_and_numpy_and_aot_performance_tests.png"}
      \caption{Wyniki testów wydajności implementacji Python z użyciem biblioteki NumPy oraz pakietu Cython do kompilacji AOT dla macierzy $\rho
      _{1}$ - $\rho_{6}$.}
      \label{python-numpy-aot-double-precision}
    \end{figure}

    Na rysunku \ref{python-numpy-aot-double-precision} przedstawione zostały wyniki
    pomiarów czasu pracy skompilowanej wersji w języku Python opierającej się na
    bibliotece NumPy wykorzystujące macierze $\rho_{1}$ - $\rho_{6}$. kompilacja nie poskutkowała
    istotnym skróceniem czasu pracy programu względem wariantu bez AOT, różnice wynoszą
    koło 1\%. Uzysk ten może być spowodowany usunięciem szczątkowego obciążenia ze
    strony interpretera, które nie jest mierzalne podczas krótszych testów z mniejszymi macierzami.
    Możliwe jest również że ta różnica wynika z korzystniejszych warunków losowo
    zapewnionych przez system operacyjny.

    \subsubsection{ Python i NumPy z JIT }
    Pomiary czasu pracy były wykonywane w taki sam sposób jak dla implementacji bez JIT.

    \begin{figure}[!ht]
      \centering
      \includegraphics
      [width=1.0\textwidth]{"resources/python_and_numpy_and_jit_performance_tests.png"}
      \caption{Wyniki testów wydajności implementacji w języku Python z użyciem biblioteki NumPy i pakietu Numba do kompilacji JIT dla macierzy $\rho
      _{1}$ - $\rho_{6}$.}
      \label{python-numpy-jit-double-precision}
    \end{figure}

    Na rysunku \ref{python-numpy-jit-double-precision} przedstawione zostały wyniki uzyskane
    podczas pomiarów czasu pracy implementacji z kompilacją JIT wykonywaną przy pomocy biblioteki
    Numba, wykorzystując macierze $\rho_{1}$ - $\rho_{6}$. Implementacja ta oferowała podczas
    testów blisko dwukrotnie krótszy czas obliczeń, względem oryginału, w przypadku macierzy
    mniejszych niż $32\times32$. Dla macierzy większych uzysk wynosił odpowiednio
    $13.8s$ ($\approx 38\%$) dla $\rho_{1}$, $28s$ ($\approx 60\%$) dla $\rho_{5}$ i $310
    .9s$ ($\approx 77\%$) dla $\rho_{6}$.

    Tak znaczącą poprawę implementacja zawdzięcza prawdopodobnie temu, że kompilator JIT
    całkowicie pozbywa się obciążenia ze strony dynamicznego systemu typów, generując wyspecjalizowany
    statycznie typowany kod maszynowy. Ponadto może on specjalizować kod dla dokładnie
    jednej platformy, korzystając z całego spektrum jej możliwości. Dotyczy to na
    przykład instrukcji SIMD, takich jak AVX2 i FMA, które są dostępne w procesorze
    użytym do testów, ale wiele wciąż popularnych procesorów ich nie posiada. Wymusza to,
    przy kompilacji AOT, zastąpienie tych instrukcji innymi szerzej dostępnymi, aby zmaksymalizować
    przenośność kodu. Dodatkowo kompilator może brać pod uwagę inne charakterystyczne
    cechy konkretnych architektur.

    \FloatBarrier

    \subsubsection{ Rust i Ndaray}


    Pomiary czasu pracy implementacji w języku Rust były wykonywane przy użyciu macierzy
    $\rho_{2}$ - $\rho_{6}$. Dane przekazywałem kolejno do programu z poleceniem
    działania w trybie FSnQd do osiągnięcia co najmniej 1000 korekcji lub do 2.000.000 iteracji
    algorytmu, w zależności od tego co nastąpi szybciej. Dla wszystkich macierzy algorytm
    uzyskał co najmniej 1000 korekcji i w żadnym przypadku nie osiągnął maksymalnej liczby
    iteracji. Dla każdej macierzy pomiar powtarzałem pięciokrotnie a wyniki uśredniłem. Pomiary
    czasu pracy dotyczyły przede wszystkim samego algorytmu\footnote{Nie biorą więc pod
    uwagę czasu pochłoniętego przez importowanie modułów itp., natomiast operacje
    wczytywania danych i pisania do plików są wliczane w czas pracy.}.

    \begin{figure}[!ht]
      \centering
      \includegraphics[width=1.0\textwidth]{"resources/rust_performance_tests.png"}
      \caption{Wyniki testów wydajności implementacji w języku Rust dla macierzy $\rho_{1}$ - $\rho
      _{6}$.}
      \label{rust-double-precision}
    \end{figure}

    Na rysunku \ref{rust-double-precision} zaprezentowane zostały wyniki pomiarów czasu pracy
    implementacja w języku Rust. Względem oryginału czas pracy dla małych macierzy
    skrócił się około pięciokrotnie, dotyczy to rozmiarów $4\times4$ i $8\times8$. W
    przypadku macierzy $16\times16$ uzysk jest już tylko dwukrotny, natomiast dla większych
    macierzy uzyskuje ona wyniki gorsze niż oryginał.

    Jest to prawdopodobnie spowodowane tym, że sama implementacja mnożenia macierzowego
    korzysta z ograniczonego zasobu instrukcji SIMD, aby zachować jak najszerszą kompatybilność
    w przeciwieństwie do biblioteki NumPy, która wewnętrznie wykorzystuje bibliotekę
    OpenBLAS\cite{NumPy_Doc}.

    \subsubsection{ Rust i Ndaray z OpenBLAS }
    Pomiary czasu pracy implementacji w języku Rust wykorzystującej OpenBLAS do wykonywania
    mnożenia macierzowego były wykonywane w taki sam sposób jak dla implementacji która nie
    korzystała z tej biblioteki.

    \begin{figure}[!ht]
      \centering
      \includegraphics[width=1.0\textwidth]{"resources/rust_blas_perf_tests.png"}
      \caption{Wyniki testów wydajności implementacji w języku Rust z użyciem biblioteki OpenBLAS dla macierzy $\rho
      _{1}$ - $\rho_{6}$.}
      \label{rust-openblas-double-precision}
    \end{figure}

    Wyniki testów przeprowadzanych na implementacji korzystającej z biblioteki OpenBLAS
    pokazują, że wykorzystanie jej poskutkowało znaczącym skróceniem czasu pracy względem
    oryginału. Zostały one zaprezentowane zostały na rysunku
    \ref{rust-openblas-double-precision}. Największa poprawa występuje dla małych macierzy,
    gdzie podobnie do wariantu nie korzystającego z OpenBLAS, przyspieszenie jest blisko
    pięciokrotne dla $\rho_{2}$, $\rho_{3}$ i ponad trzykrotne dla $\rho_{2}$. Dla macierzy
    $32\times32$ i $64\times64$ obliczenia zajęły około dwukrotnie mniej czasu. Pokazuje
    to jak znaczący wzrost wydajności można uzyskać przy pomocy wyspecjalizowanego kodu
    w języku Asemblera, który został wykorzystany w bibliotece OpenBLAS do stworzenia implementacji
    mnożenia macierzowego wykorzystujących możliwie największą część możliwości CPU.

    \FloatBarrier

    \subsection{Pomiary z pojedynczą precyzją}
    \label{plots-single-precision} Wszystkie testy wydajności dla obliczeń
    wykorzystujących liczby zmiennoprzecinkowe pojedynczej precyzji były przeprowadzane
    w taki sam sposób jak odpowiadające testy z podwójną precyzją.

    \subsubsection{ Python i NumPy }
    \begin{figure}[!ht]
      \centering
      \includegraphics
      [width=1.0\textwidth]{"resources/python_and_numpy_single_tests.png"}
      \caption{Wyniki testów wydajności implementacji w języku Python z użyciem biblioteki NumPy dla macierzy $\rho
      _{1}$ - $\rho_{6}$ i obliczeniami pojedynczej precyzji.}
      \label{sp-numpy-perf}
    \end{figure}

    Na wykresie \ref{sp-numpy-perf} przedstawiłem wyniki wydajności dla implementacji napisanej
    w języku Python wykorzystującej bibliotekę NumPy do przeprowadzania obliczeń na macierzach
    liczb zespolonych pojedynczej precyzji. W przypadku mniejszych macierzy ($4\times4$,
    $8\times8$, $16\times16$) różnice w czasie pracy, względem wariantu opartego na liczbach
    podwójnej precyzji, są minimalne. Dzieje się tak prawdopodobnie dlatego, że macierze
    te są na tyle niewielkie (do 4KB) że mieszczą się w pamięci cache L1 procesora\footnote{Wykorzystywany
    tutaj Ryzen 9 7950X posiada $32\times16$KB cache L1}, więc wymnażanie ich jest
    procesem bardzo szybkim. W momencie kiedy docieramy do macierzy $32\times32$ wzrost wydajności
    staje się zauważalny, co również można wytłumaczyć odwołując się do pojemności
    pamięci cache procesora. Macierze podwójnej precyzji zajmują dokładnie 16KB ($32\times
    32\times2\times 8$), natomiast dostęp do tej pamięci nie jest ekskluzywny dla
    jednego procesu, nie może on więc korzystać z całych 16KB. W efekcie część danych
    przebywa poza pamięcią cache. Natomiast macierze wykorzystujące liczby pojedynczej precyzji
    zajmują tylko 8KB. Można się więc spodziewać że większość czasu spędzają one w
    pamięciach L1 i L2, co pozwala przyspieszyć obliczenia. Dodatkowo mniejszy rozmiar
    liczb pojedynczej precyzji pozwala dwukrotnie zwiększyć przepustowość instrukcji
    SIMD, co prawdopodobnie odgrywa również bardzo istotną rolę, szczególnie w przypadku
    macierzy $64\times64$, dla których obliczenia przyspieszają znacznie bardziej niż w przypadku
    mniejszych macierzy.

    \subsubsection{ Python i NumPy z AOT }
    \begin{figure}[!ht]
      \centering
      \includegraphics
      [width=1.0\textwidth]{"resources/python_and_numpy_and_aot_single_tests.png"}
      \caption{Wyniki testów wydajności implementacji w języku Python z użyciem biblioteki NumPy i pakietu Cython do kompilacji AOT dla macierzy $\rho
      _{1}$ - $\rho_{6}$ i obliczeniami pojedynczej precyzji.}
      \label{python-numpy-cython-single-precision}
    \end{figure}

    Wyniki dla wariantu pre-kompilowanego przy pomocy biblioteki Cython nie różnią się
    znacząco od wariantu nie pre-kompilowanego, podobnie jak w przypadku obliczeń podwójnej
    precyzji, zostały one zaprezentowane na rysunku
    \ref{python-numpy-cython-single-precision}. Podobnie jak w przypadku wyników dla obliczeń
    podwójnej precyzji, pre-kompilacja nie przynosi istotnych zysków wydajnościowych.

    \subsubsection{ Python i NumPy z JIT }
    \begin{figure}[!ht]
      \centering
      \includegraphics
      [width=1.0\textwidth]{"resources/python_and_numpy_and_jit_single_tests.png"}
      \caption{Wyniki testów wydajności implementacji w języku Python z użyciem biblioteki NumPy i pakietu Numba do kompilacji JIT dla macierzy $\rho
      _{1}$ - $\rho_{6}$ i obliczeniami pojedynczej precyzji.}
      \label{sp-numpy-jit-perf}
    \end{figure}

    W przypadku wariantu wykorzystującego kompilację JIT, zysk czasowy wynikający z
    redukcji precyzji obliczeń jest minimalny lub wręcz nie występuje. Wyjątkiem są tutaj
    wyniki dla macierzy $\rho_{1}$ w przypadku której czas pracy skrócił się o $4.7s$ ( $\approx
    21\%$).

    \FloatBarrier

    \subsubsection{ Rust i Ndaray}
    \begin{figure}[!ht]
      \centering
      \includegraphics[width=1.0\textwidth]{"resources/rust_single_tests.png"}
      \caption{Wyniki testów wydajności implementacji w języku Rust z użyciem biblioteki Ndarray dla macierzy $\rho
      _{1}$ - $\rho_{6}$ i obliczeniami pojedynczej precyzji.}
      \label{sp-rust-perf}
    \end{figure}

    Implementacja w języku Rust wykorzystująca bibliotekę Ndarray prezentuje znaczną
    poprawę wydajności dla wszystkich wymiarów macierzy. Podczas obliczeń na małych macierzach,
    do $16\times16$ włącznie, uzyskuje ona najlepsze wyniki w zestawieniu, natomiast dla
    większych macierzy poprawa występuje, ale ciągle implementacja w języku Python z JIT
    daje lepsze efekty.

    \subsubsection{ Rust i Ndaray z OpenBLAS }
    \begin{figure}[!ht]
      \centering
      \includegraphics[width=1.0\textwidth]{"resources/rust_blas_single_tests.png"}
      \caption{Wyniki testów wydajności implementacji w języku Rust z użyciem biblioteki Ndarray dla macierzy $\rho
      _{1}$ - $\rho_{6}$ i obliczeniami pojedynczej precyzji.}
      \label{sp-rust-blas-perf}
    \end{figure}

    Zestawienie języka Rust i biblioteki Ndaray z pakietem OpenBLAS i liczbami
    zmiennoprzecinkowymi pojedynczej precyzji poskutkowało uzyskaniem znaczącej poprawy
    wyników wydajności, które zostały przedstawione na rysunku \ref{sp-rust-blas-perf}.
    W przypadku macierzy $4\times4$ i $8\times8$ wydajność jest bardzo zbliżona do
    wariantu nie korzystającego z OpenBLAS, natomiast wraz ze wzrostem rozmiaru macierzy,
    skrócenie czasu pracy staje się coraz bardziej widoczne. Względem oryginału,
    obliczenia dla macierzy:
    \begin{itemize}
      \item $\rho_{1}$ trwają $\approx 5.8\times$ krócej,

      \item $\rho_{5}$ trwają $\approx 6.4\times$ krócej,

      \item $\rho_{6}$ trwają $\approx 7.4\times$ krócej,
    \end{itemize}
    Biorąc pod uwagę zyski wydajności, wynikające z obniżenia precyzji obliczeń, dla pozostałych
    implementacji, tak istotne skrócenie czasu pracy jest zaskakujące. Z tego względu powtórnie
    upewniłem się, że praca programu kończy się uzyskaniem odpowiednich wyników
    liczbowych i nie wykryłem żadnych nieprawidłowości.

    \FloatBarrier

    \subsection{Zestawienia dla macierzy}
    W tej sekcji prezentuję wyniki tych samych pomiarów co w sekcjach \ref{plots-double-precision}
    i \ref{plots-single-precision} zestawiając je ze sobą względem macierzy wykorzystanej
    do testów.

    Oznaczenie `(F64)' przy nazwie implementacji oznacza obliczenia z podwójną precyzją,
    analogicznie `(F32)' oznacza obliczenia z pojedynczą precyzją. `Oryginał' to kod implementacji
    autorstwa dr hab. Marcin Wieśniak, prof. UG, natomiast pozycja podpisana `Oryginał (zablokowany)'
    to ten sam kod ale z zablokowaną liczbą wątków obliczeniowych.

    \subsubsection{Macierz \texorpdfstring{$\rho_{1}$ $(32\times32)$}{rho1 32x32}}
    \begin{figure}[!ht]
      \centering
      \includegraphics[width=1.0\textwidth]{"resources/rho_1_matrix_comparison.png"}
      \caption{Zestawienie wyników testów wydajności wszystkich implementacji dla macierzy $\rho
      _{1}$.}
      \label{matrix-comparison-rho-1-plot}
    \end{figure}

    Na rysunku \ref{matrix-comparison-rho-1-plot} przedstawione zostało zestawienie
    wyników wydajności dla testów przeprowadzanych przy pomocy macierzy $\rho_{1}$. Najkrótszy
    czas pracy w zestawieniu, $17.9s$, uzyskała implementacja napisana w języku Python z
    użyciem biblioteki NumPy z kompilacją JIT wykonująca obliczenia pojedynczej precyzji.
    Niewiele ustępuje jej wariant pracujący na liczbach podwójnej precyzji z wynikiem
    $22 .6s$.

    \subsubsection{Macierz \texorpdfstring{$\rho_{2}$ $(4\times4)$}{rho2 4x4}}
    \begin{figure}[!ht]
      \centering
      \includegraphics[width=1.0\textwidth]{"resources/rho_2_matrix_comparison.png"}
      \caption{Zestawienie wyników testów wydajności wszystkich implementacji dla macierzy $\rho
      _{2}$.}
      \label{matrix-comparison-rho-2-plot}
    \end{figure}

    Na rysunku \ref{matrix-comparison-rho-2-plot} przedstawione zostało zestawienie wyników
    wydajności dla testów przeprowadzanych przy pomocy macierzy $\rho_{2}$. Najkrótszy
    czas pracy w zestawieniu, $0.5s$, uzyskała implementacja napisana w języku Rust
    wykonująca obliczenia pojedynczej precyzji. Bardzo zbliżony wynik uzyskał wariant korzystający
    z biblioteki OpenBLAS$0.6s$.

    Z pośród implementacji w języku Python najlepiej sprawował się wariant z JIT (F64), natomiast
    jego czas pracy był około $7.6\times$ dłuższy.

    \subsubsection{Macierz \texorpdfstring{$\rho_{3}$ $(8\times8)$}{rho3 8x8}}
    \begin{figure}[!ht]
      \centering
      \includegraphics[width=1.0\textwidth]{"resources/rho_3_matrix_comparison.png"}
      \caption{Zestawienie wyników testów wydajności wszystkich implementacji dla macierzy $\rho
      _{3}$.}
      \label{matrix-comparison-rho-3-plot}
    \end{figure}
    Na rysunku \ref{matrix-comparison-rho-3-plot} przedstawione zostało zestawienie wyników
    wydajności dla testów przeprowadzanych przy pomocy macierzy $\rho_{3}$. Najkrótszy
    czas pracy w zestawieniu, $0.6s$, uzyskała implementacja napisana w języku Rust z
    OpenBLAS wykonująca obliczenia pojedynczej precyzji. Bardzo zbliżony wynik uzyskał wariant
    który nie korzystał z OpenBLAS ($0.9s$).

    \subsubsection{Macierz \texorpdfstring{$\rho_{4}$ $(16\times16)$}{rho4 16x16}}
    \begin{figure}[!ht]
      \centering
      \includegraphics[width=1.0\textwidth]{"resources/rho_4_matrix_comparison.png"}
      \caption{Zestawienie wyników testów wydajności wszystkich implementacji dla macierzy $\rho
      _{4}$.}
      \label{matrix-comparison-rho-4-plot}
    \end{figure}
    Na rysunku \ref{matrix-comparison-rho-4-plot} przedstawione zostało zestawienie wyników
    wydajności dla testów przeprowadzanych przy pomocy macierzy $\rho_{4}$. Najkrótszy
    czas pracy w zestawieniu, $1.5s$, uzyskała implementacja napisana w języku Rust z
    OpenBLAS wykonująca obliczenia pojedynczej precyzji. Bardzo zbliżony wynik uzyskał wariant
    który nie korzystał z OpenBLAS ($4s$).

    \subsubsection{Macierz \texorpdfstring{$\rho_{5}$ $(32\times32)$}{rho5 32x32}}
    \begin{figure}[!ht]
      \centering
      \includegraphics[width=1.0\textwidth]{"resources/rho_5_matrix_comparison.png"}
      \caption{Zestawienie wyników testów wydajności wszystkich implementacji dla macierzy $\rho
      _{5}$.}
      \label{matrix-comparison-rho-5-plot}
    \end{figure}
    Na rysunku \ref{matrix-comparison-rho-5-plot} przedstawione zostało zestawienie wyników
    wydajności dla testów przeprowadzanych przy pomocy macierzy $\rho_{5}$. Najkrótszy
    czas pracy w zestawieniu, $7.2s$, uzyskała implementacja napisana w języku Rust z
    OpenBLAS wykonująca obliczenia pojedynczej precyzji. Następny najniższy wynik wynosił
    $18.2s$ i był osiągany przez wariant w języku Python korzystający z JIT, niezależnie
    od precyzji obliczeń.

    \FloatBarrier

    \subsubsection{Macierz \texorpdfstring{$\rho_{6}$ $(64\times64)$}{rho6 64x64}}
    \begin{figure}[!ht]
      \centering
      \includegraphics[width=1.0\textwidth]{"resources/rho_6_matrix_comparison.png"}
      \caption{Zestawienie wyników testów wydajności wszystkich implementacji dla macierzy $\rho
      _{6}$.}
      \label{matrix-comparison-rho-6-plot}
    \end{figure}
    Na rysunku \ref{matrix-comparison-rho-6-plot} przedstawione zostało zestawienie wyników
    wydajności dla testów przeprowadzanych przy pomocy macierzy $\rho_{6}$. Najkrótszy
    czas pracy w zestawieniu, $54.8s$, uzyskała implementacja napisana w języku Rust z
    OpenBLAS wykonująca obliczenia pojedynczej precyzji. Następny najniższy wynik wynosił
    $93.1s$ i był osiągany przez wariant w języku Python korzystający z JIT przy obliczeniach
    podwójnej precyzji. Kod wykonujący obliczenia pojedynczej precyzji wypadł
    nieznacznie gorzej.

    \FloatBarrier

    \subsection{Profilowanie Rust z OpenBLAS}
    Po przeprowadzeniu testów wydajności uznałem, że konieczne jest wykonanie
    profilowania na najwydajniejszej implementacji, aby sprawdzić czy i gdzie istnieje jeszcze
    szansa na poprawę. Dlatego też przygotowałem odpowiedni program pomocniczy który
    wywoływał implementację w języku Rust z OpenBLAS i pojedynczą precyzją bez
    konieczności angażowania interpretera. Było to rozwiązanie, które zarówno pozwalało
    usunąć duże ilości zbędnych informacji z danych profilowania. Do profilowania wykorzystałem
    macierz $64\times64$.

    \begin{figure}[!ht]
      \centering
      \includegraphics[width=1.0\textwidth]{"resources/rust_flame_graph_perf.png"}
      \caption{Wizualizacja charakterystyki zachowania implementacji Rust i Ndaray z OpenBLAS podczas obliczeń pojedynczej precyzji.}
      \label{rust-flame-graph-prof}
    \end{figure}

    Wizualizacja typu flame graph wyników tego profilowania została umieszczona na
    rysunku \ref{rust-flame-graph-prof}. Wykres przedstawia dane analogicznie do wykresy
    Icircle z rysunku \ref{pre-prof-perf}, natomiast robi to w przeciwnym kierunku -
    pierwsze wywołania są na dole, a te najbardziej zagnieżdżone na górze. Obraz został
    stworzony przy pomocy programu Hotspot\cite{HOTSPOT} rozwijanego przez firmę KDAB.

    Na podstawie zaprezentowanego wykresu wywnioskować można, że większość czasu pracy
    programu - około 95\% - pochłaniają mnożenia macierzowe wykonywane w obrębie funkcji
    \code{rotate()}. Operacje te wykonywane są przy pomocy funkcji z biblioteki OpenBLAS.
    Korzysta ona z implementacji napisanych w języku Asemblera, które są specjalnie optymalizowane
    by wykorzystywać maksimum możliwości sprzętowych. Tym samym, nie ma prostego sposobu,
    by kontynuować proces optymalizacji czasu wykonania programu.

    \FloatBarrier

    \section{Dyskusja}
    \subsection{Podsumowanie}
    Praca ta miała na celu eksplorację dostępnych metod maksymalizacji wydajności algorytmu
    CSSF. Wykonałem badanie skuteczności wykorzystania pięciu wariantów programu, w tym implementacji
    w dwóch różnych językach programowania. Algorytm CSSF został z powodzeniem
    wielokrotnie zaimplementowany, a wszystkie stworzone implementacje pozwalały uzyskać
    oczekiwane wyniki liczbowe.

    Przeprowadziłem również liczne testy wydajności, dla różnych danych wejściowych, dla
    wszystkich wariantów programu, co pozwoliło mi ustalić które metody okazały się
    najbardziej skuteczne. Ponadto, każdy z wariantów testowałem zarówno podczas
    obliczeń pojedynczej jak i podwójnej precyzji. Pozwoliło mi to zbadać wpływ precyzji
    obliczeń na wydajność kodu. W przypadku dwóch wariantów programu, udało mi się uzyskać
    znaczącą poprawę wydajności.

    Po przeprowadzeniu analizy uzyskanych wyników udało mi się wskazać, że warianty:
    \begin{enumerate}
      \item Python i NumPy z JIT niezależnie od precyzji,

      \item Rust i Ndarray z OpenBLAS, dla obliczeń pojedynczej precyzji,
    \end{enumerate}

    Oferujące znaczące skrócenie czasu pracy względem oryginału. Dla macierzy $64\times64$,
    a więc układów 6 kubitów, czas pracy pierwszego wariantu skrócił się względem oryginału
    $4.3$ raza, natomiast w przypadku drugiego $7.4$ raza.

    Warto podkreślić, że wydajność obu wariantów jest zbliżona, natomiast nakład pracy
    konieczny do stworzenia implementacji w języku Rust był nieporównywalnie większy niż
    ten potrzebny do dodania kompilacji JIT do kodu w języku Python. Podobnych wyników można
    spodziewać się w przypadku wielu innych programów skupiających się na obliczeniach macierzowych
    - kompilacja JIT jest w stanie, dość skutecznie, usunąć wady języka Python. Wymaga
    to poświęcenia pewnej części swobody, którą ten język daje, ale ciągle pozostaje jej
    na tyle dużo, że proces pisania kodu jest mniej czasochłonny niż w przypadku języków
    niskopoziomowych. Oczywiście, języki te dają większą kontrolę nad komputerem i pozwalają
    na wiele błyskotliwych, manualnych optymalizacji, tak jak ma się to w przypadku OpenBLAS,
    natomiast użycie tej biblioteki w projekcie na ogół pozwala na wykorzystanie
    większości możliwości procesora, bez konieczności pisania niskopoziomowego kodu.

    Kolejnym ważnym spostrzeżeniem jest, że cztery z pięciu zaprezentowanych
    implementacji wykorzystywały do wykonywania mnożenia macierzowego bibliotekę
    OpenBLAS. Moduł Pythona NumPy wykorzystuje ją zawsze, natomiast w pakiecie języka
    Rust, Ndarray, możliwe jest ręczne włączenie takiego zachowania. Pomimo tego różnice
    w wydajności pomiędzy nimi są bardzo znaczące. Działo się tak ponieważ znaczącą
    część czasu wykonywania kodu zajmowały inne operacje, których zaimplementowany w Asemblerze
    OpenBLAS przyspieszyć nie mógł.

    \subsection{Kontynuacja}
    Praca ta nie wyczerpuje całej puli narzędzi, które można wykorzystać do
    implementacji algorytmu CSSF. W przyszłości można sprawdzić jakie efekty dałoby wykorzystanie
    GPU\footnote{GPU (Graphics Processing Unit) to wyspecjalizowany układ scalony
    przeznaczony do szybkiego i efektywnego przetwarzania obrazów w celu wyświetlenia
    ich na urządzeniu wyjściowym, takim jak monitor komputerowy. GPU jest
    zoptymalizowany do wykonywania obliczeń równoległych, które są niezbędne do renderowania
    grafiki 3D i 2D, a także jest często wykorzystywany w innych intensywnych
    obliczeniowo dziedzinach, takich jak przetwarzanie danych, sztuczna inteligencja czy
    uczenie maszynowe, ze względu na swoją zdolność do przetwarzania dużej ilości danych
    jednocześnie\cite{CPU_VS_GPU}.} do przeprowadzania obliczeń oraz jakie metody
    implementacji tych obliczeń dają najlepsze efekty. Dobrze byłoby w takim wypadku rozważyć
    możliwość skorzystania z biblioteki CuPy, CUDA czy też funkcjonalności biblioteki
    Numba pozwalających na wykorzystanie GPU. Istnieje też możliwość skorzystania z Vulkan
    API poprzez język Rust lub C++ i compute shaderów\footnote{shader - program
    wykonywany na GPU.} napisanych w języku GLSL. Istnieje szansa, że algorytm w całości
    wykonywany na GPU, bez transferów na CPU byłby w stanie zaoferować interesujące wyniki
    i lepsze skalowanie w przypadku większych macierzy stanu.

    \subsection{Pominięte narzędzia}
    W zestawieniu zawartym w tej pracy pod uwagę wzięty został ograniczony podzbiór możliwych
    metod implementacji algorytmu CSSF. W przypadku wielu pominiętych metod, wykluczenie
    ich wynikało z ograniczeń czasowych. Jednakże niektóre z narzędzi zostały świadomie
    odrzucone na etapie planowania, ponieważ istniały dla nich lepsze alternatywy.

    \subsubsection{PyTorch i TensorFlow}
    Biblioteki TensorFlow i PyTorch zostały stworzone, aby ułatwić prace badaczy i
    inżynierów pracujących nad uczeniem maszynowym i głębokim uczeniem. W obu znajdują się
    implementacje tensorów oferujących pewien podzbiór funkcjonalności obiektów ndarray z
    biblioteki NumPy. Prawdopodobnie zawierają one wystarczająco wiele elementów by
    zaimplementować przy ich pomocy algorytm CSSF. Posiadają one również bardzo rozbudowany
    bagaż innych elementów, które są bardzo pomocne gdy moduły te są używane do
    tworzenia sieci neuronowych, ale są zupełnie zbędne dla programu CSSFinder.
    Jednocześnie biblioteka NumPy jest rozwiązaniem bardziej powszechnym w podobnych
    scenariuszach i prawdopodobnie oferującym najlepszą możliwą przepustowość
    obliczeniową, możliwą do uzyskania na CPU, biorąc pod uwagę że wewnętrznie używa ona
    OpenBLAS. Uznałem, że czas konieczny do zaimplementowania algorytmu CSSF przy pomocy
    PyTorch czy TensorFlow będzie niewspółmierny do spodziewanych zysków wydajnościowych,
    więc lepiej czas ten poświęcić na rozważenie rozwiązań, które rokują lepiej.

    \subsubsection{PyPy}
    PyPy jest alternatywną do CPythona implementacją języka Python. Oferuje ona wbudowany,
    automatyczny tracing JIT compiler. Jednocześnie w dokumentacji tego interpretera zaznaczane
    jest, że najlepiej sprawuje się on z kodem który jest napisany w większości w języku
    Python, a pakiety takie jak NumPy, napisane m. in. w C będą oferowały słabą
    wydajność\cite{PyPyPerformance}. Ponadto w obecnym momencie wymaga on dedykowanych
    pakietów wheel podczas instalacji pakietów z PyPI, co stwarza dodatkowy problem podczas
    instalacji zależności, które wykorzystują moduły rozszerzeń skompilowane do kodu
    maszynowego. Ponownie uderza to w NumPy. Biorąc pod uwagę że biblioteka Numba, która
    również oferuje kompilację JIT została stworzona do współpracy z NumPy i w takich scenariuszach
    radzi sobie najlepiej, zdecydowałem o porzuceniu prób wykorzystania PyPy.

    \subsubsection{C/C++}
    Języki C i C++ nie zostały ujęte w zestawieniu, zamiast nich ujęty został język Rust.
    Wszystkie trzy języki są kompilowane do kodu maszynowego. W przypadku środowiska którym
    posługiwałem się podczas prac programistycznych wszystkie trzy byłyby kompilowane przy
    pomocy LLVM, więc można oczekiwać, że wykazywałyby one zbliżoną wydajność.

    Rust posiada szereg zalet, które skłoniły mnie by go wykorzystać. Oferuje on wygodny
    i standaryzowany ekosystem który pozwala w szybki sposób rozpocząć prace programistyczne,
    a potem prowadzić je bez problemów związanych z kompilacją i obsługą zależności. Dodatkowo
    posiada on system bezpiecznego zarządzania pamięcią, który uniemożliwia programiście
    popełnienie błędów z nią związanych, jak wielokrotne dealokacje czy wyciek pamięci.
    Powoduje to, że kod napisany w języku Rust jest bardziej niezawodny, co w przypadku
    projektu realizowanego w ramach pracy dyplomowej jest ważnym elementem.

    Języki C i C++ podobnego ekosystemu nie posiadają, więc wymagałyby ręcznego stworzenia
    konfiguracji systemu budowania, prawdopodobnie w oparciu o CMake, zebrania bibliotek
    i odkrycia w jaki sposób połączyć je z konfiguracją systemu budowania oraz odpowiedniego
    spreparowania środowiska. Wszystko to byłoby znacznie bardziej pracochłonne niż w
    przypadku języka Rust. Doszedłem więc do wniosku, że za ilością czasu konieczną do
    stworzenia implementacji w oparciu o te języki nie idą żadne istotne korzyści.

    \section{Wnioski}
    \subsection{Podsumowanie}
    Dwa z pośród pięciu stworzonych przeze mnie wariantów kodu wykazały się znaczącą
    poprawą wydajności względem oryginału. Najlepszą wydajność reprezentowały warianty:
    \begin{itemize}
      \item napisany w języku Python, wykorzystujący bibliotekę NumPy i bibliotekę Numba
        pozwalającą na wykonywanie kompilacji JIT kodu Pythona, niezależnie od precyzji obliczeń
        (do $4.3\times$ szybszy dla układów 6 kubitów),

      \item napisany w języku Rust, wykorzystujący biblioteki Ndarray i OpenBLAS, tylko w
        przypadku obliczeń pojedynczej precyzji. (do $7.4\times$ szybszy dla układów 6 kubitów)
    \end{itemize}
    W kontekście otrzymanych wyników warto podkreślić, że zaimplementowanie wariantu pierwszego
    było procesem znacznie szybszym, niż stworzenie wariantu drugiego, a wariant drugi oferuje
    tylko około 30\% krótszy czas pracy.

    Z informacji uzyskanych podczas profilowania wariantu Rust z OpenBLAS wynika, że około
    95\% czasu pracy pochłaniają wysoce zoptymalizowane mnożenia macierzowe wykonywane
    przez OpenBLAS. Uznaję więc, że wykorzystałem większość potencjału optymalizacyjnego
    i prawdopodobnie nie jestem w stanie, z użyciem posiadanego przeze mnie sprzętu i oprogramowania,
    uzyskać dalszego znaczącego skrócenia czasu pracy.

    \subsection{Kod}
    Stworzony przeze mnie kod został zamieszczony w trzech repozytoriach w serwisie
    GitHub:
    \begin{enumerate}
      \item cssfinder\cite{CSSFinder_New},

      \item cssfinder\_backend\_numpy\cite{CSSFinder_New_Numpy},

      \item cssfinder\_backend\_rust\cite{CSSFinder_New_Rust},
    \end{enumerate}

    Dla każdego z tych repozytorium istnieje odpowiadający pakiet menadżera pakietów pip\cite{PIP}
    zamieszczony na serwerze PyPI. Zostały one utworzone w sposób zgodny z ekosystemem
    języka Python. Pozwala to w bardzo prosty sposób rozpocząć korzystanie z programu poprzez
    instalację następujących pakietów:
    \begin{enumerate}
      \item cssfinder\cite{CSSFinder_New_PyPI},

      \item cssfinder\_backend\_numpy\cite{CSSFinder_New_Numpy_PyPI},

      \item cssfinder\_backend\_rust\cite{CSSFinder_New_Rust_PyPI},
    \end{enumerate}

    Pakiety są kompatybilne z systemami Linux, MacOS i Windows. Wymagają interpretera języka
    Python w wersji 3.8 lub nowszej.
  \end{sloppypar}
  \newpage


  \begin{sloppypar}
    \medskip
    \printbibliography
    [heading=bibintoc, title={Odwołania}]
  \end{sloppypar}

  \newpage


  \begin{sloppypar}
    \raggedleft \normalsize Załącznik nr 3 do zarządzenia Rektora UG nr 70/R/15

    \vspace{2.5cm}


    \centering
    \Large \textbf{OŚWIADCZENIE}

    \vspace{2.5cm}


    Wyrażam zgodę na udostępnienie osobom zainteresowanym mojej pracy dyplomowej dla
    celów naukowo-badawczych. Zgoda na udostępnienie pracy dyplomowej nie oznacza wyrażenia
    zgody na kopiowanie pracy dyplomowej w całości lub w części.

    \vspace{2.5cm}
    \centering


    \hspace{1.2cm} 26.06.2023 \hspace{6.5cm}
    \includegraphics[height=1cm]{"resources/signature.jpg"}
    \hspace{3.5cm} \\ \hspace{0.7cm} \dotfill \hspace{3cm} \dotfill \hspace{1cm} \\
    \hspace{1cm} \small{data} \hspace{8cm} \small{podpis}
  \end{sloppypar}

  \newpage


  \begin{sloppypar}
    \raggedleft \normalsize Załącznik nr 1 do zarządzenia Rektora UG nr 70/R/15 ze zm.

    \vspace{2.5cm}


    \centering
    \Large \textbf{OŚWIADCZENIE}

    \vspace{2.5cm}


    Oświadczam, że przedłożona praca dyplomowa została przygotowana przeze mnie
    samodzielnie, nie narusza praw autorskich, interesów prawnych i materialnych innych
    osób oraz wykorzystanie materiałów wytworzonych przez generatywne narzędzia
    sztucznej inteligencji odbyło się w zakresie uzgodnionym z promotorem.

    \vspace{2.5cm}
    \centering


    \hspace{1.2cm} 26.06.2023 \hspace{6.5cm}
    \includegraphics[height=1cm]{"resources/signature.jpg"}
    \hspace{3.5cm} \\ \hspace{0.7cm} \dotfill \hspace{3cm} \dotfill \hspace{1cm} \\ \hspace{1cm}
    \small{data} \hspace{8cm} \small{podpis}
  \end{sloppypar}
\end{document}